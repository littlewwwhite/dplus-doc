# 1 **预研清单**

## (1) 针对单栏PDF的语义切分。语义切分包括标题提取，标题树构建，标题-正文映射

[也谈 langchain 大模型外挂知识库问答系统核心部件：如何更好地解析、分割复杂非结构化文本](https://mp.weixin.qq.com/s/rOWfCQuUPohatMF_dU2nIA)


## (2) 对几十万字的PDF进行文档摘要生成，方案研究，以及验证需要多长时间

### 1、模型改进：


#### Longformer（专用于大模型中长文本的transformer结构，本质上做改进）

**论文地址：** https://arxiv.org/abs/2309.12307

**代码和 Demo 地址：**  https://github.com/dvlab-research/LongLoRA

中文讲解：[一键读完《三体》！港中文联 MIT 破解 AI 遗忘魔咒，2 行代码引领超长文本革命！](https://mp.weixin.qq.com/s/Iz6nMWU6XofJzY65Ojk-KQ)


#### streamingLLM：（meta-ai)


* https://github.com/mit-han-lab/streaming-llm
* 中文讲解：[LLM 对无限长的文本序列进行语言建模：处理超过 400 万个 token 的文本](https://mp.weixin.qq.com/s/WtSGtMDseIml7WViMvJhZg)
* 跟图谱的结合：[国庆假期收尾之巧用注意力汇聚高效处理长文本思路 StreamingLLM：兼观 CCKS2023 知识图谱与大模型的一些前沿趋势](https://mp.weixin.qq.com/s/i7ezkp4IEEshLvlotphzOg)


#### rwkv模型，linear transformer


[大语言模型时代的长文本处理讨论 (4000 字)](https://mp.weixin.qq.com/s/TZ9VXS7QuxRMc5rAikz-TQ)


### 2、外部改进：LongChat

[如何让大模型处理更长的文本？](https://mp.weixin.qq.com/s/IBUbFuEpABiEISXMQKys8A)

类似于在外部将输入做改进的方法还有很多，这里不一一列举。



## (3) OA分析助手意图理解部分，验证当前方案能否实现预期功能，如果不行，找寻新的方案并验证


### 1、 阿里小密意图识别

[基于知识增强和预训练大模型的 Query 意图识别](https://mp.weixin.qq.com/s/lVGKwNDgaHLROPdN3XUmiw)


### 对大模型做意图识别任务的理解


意图识别表面上就是个因地制宜问题，但领域和意图的划分规则非常多样，而且划分很多时候不是主观的，有很多客观因素存在，列举一下可能会考虑到的地方：

* 是否有用模型的必要，有些规则或者词典就能轻松做到的，别说大模型，模型都没必要，而且甚至还没规则词典做的好。
* 泛化能力的需求是用模型的门槛，有泛化需求才有用模型的必要。
* 边界是否能比较简单地陈述。毕竟大模型的分类依赖指令和 few-shot 样本，如果不好描述，其实很难做好这种分类，例如 “百科”、“客服”、“投诉” 这些，语言其实很难描述他们的边界，此时大模型做起来其实也会比较吃力。
* 是否有足够的训练数据。训练数据越充裕，小模型的优势越会体现。
* 数据不足的时候，few-shot（自己编几个），in-context learning 可能也可以取得不错的效果，就看别的因素了，随着数据量增加，结合向量召回找到合适的样本再用 in-context learning，也会有不错的效果。
* 性能，性能，还是性能。尤其是意图识别本身只是搜索中的小部分，耗时还是要给召回、排序让更多的。

若非找资源充足且性能要求不高，或者有技术影响力之类的压力，一般不会直接用大模型直接做意图识别，更多可能的思路：

* 大模型做 baseline 快速预测，构造标签样本。
* 直接生成泛化样本供小模型训练使用。

所以在这种小的、比较经典的 NLP 任务下，大模型的发挥空间其实不那么大，杀鸡焉用牛刀。而且在实用过程中：

* 简单任务，大模型确实能得到更高的下限，但是后续要调优，肯定是分离出来用小模型专项优化，提升会更多。
* 复杂任务，大模型需要复杂指令，同时也要 fewshot 起步，否则模型压根不知道怎么分，这也合理，毕竟边界模糊真的不好讲述，用样本来描述边界会更加明显，这个时候大模型就体现不出优势了。


(4) PROPMT设计并验证开源模型是否满足，如果不满足则进行改进。其中的Propmt主要是方案2.7和2.8中设计的propmt
