{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实体名称泛化失败和属性识别\n",
    "\n",
    "- 目前解决思路：利用分词或者向量匹配等方法将其中的实体拼接起来作为prompt给大模型来识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "medical_data = \"疾病名称,疾病描述,疾病种类,科室,病因,症状,检查,并发症,花费,疗程,疗法,治愈率,易感人群,感染概率,感染途径,预防措施,推荐药物,常用药物,具体药物,可以吃,不可以吃,推荐吃,是否纳入医保\"\n",
    "with open('/Users/dingzhijian/VSCode/dplus-doc/nl2cypher/Medical Data.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    schema = []\n",
    "    # 取出第一列数据\n",
    "    for row in reader:\n",
    "        schema.append(row['疾病名称'])\n",
    "    print(schema)\n",
    "\n",
    "\n",
    "medical_data = \"疾病名称,疾病描述,疾病种类,科室,病因,症状,检查,并发症,花费,疗程,疗法,治愈率,易感人群,感染概率,感染途径,预防措施,推荐药物,常用药物,具体药物,可以吃,不可以吃,推荐吃,是否纳入医保\"\n",
    "medical_terms_list = medical_data.split(',')\n",
    "print(medical_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 结巴分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n",
      "[2023-11-14 19:37:47,195] [   DEBUG] _compat.py:47 - Paddle enabled successfully......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paddle Mode: 哪些/人/易/感染/肺放线菌病/?\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import paddle\n",
    "\n",
    "paddle.enable_static()\n",
    "jieba.enable_paddle()# 启动paddle模式。 0.40版之后开始支持，早期版本不支持\n",
    "strs=[\"哪些人易感染肺放线菌病?\"]\n",
    "for str in strs:\n",
    "    seg_list = jieba.cut(str,use_paddle=True) # 使用paddle模式\n",
    "    print(\"Paddle Mode: \" + '/'.join(list(seg_list)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. fuzz字段模糊匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('易感', 90), ('容易感染', 50), ('容易患上', 25)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fuzzy查询\n",
    "from thefuzz import fuzz\n",
    "from thefuzz import process\n",
    "\n",
    "# 注意方法，现在有两种做法\n",
    "#   1. 直接整句来对比\n",
    "#   2. 只用实体来对比（更好）\n",
    "\n",
    "sentences = ['肺泡蛋白质沉积症', '肺泡蛋白质沉淀' ,'肺泡蛋白整', '肺泡蛋白沉着症','大页性肺炎', '肺泡里沉淀蛋白', '放射性肺炎', '肺念珠菌病', '肺大疱', '肺炎球菌肺炎', '肺气肿', '肺炎杆菌肺炎']\n",
    "tokens_input = ['易感','容易感染','容易患上']\n",
    "\n",
    "process.extract(\"易感人群\", tokens_input, limit=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 向量匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer('/dataQ/zhijian/LLMs/m3e-base')\n",
    "tokens_input = ['肺泡蛋白质沉积症', '肺泡蛋白质沉淀' , '肺泡蛋白沉着症', '肺泡沉淀有蛋白质', '放射性肺炎', '肺念珠菌病', '肺大疱', '肺炎球菌肺炎', '肺气肿', '肺炎杆菌肺炎']\n",
    "tokens_dict = ['易感','容易感染','容易患上','易感人群','容易得']\n",
    "sentences3 = []\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "\n",
    "sentences = seg_list\n",
    "embeddings = model.encode(sentences)\n",
    "embeddings3 = model.encode(sentences3)\n",
    "\n",
    "sent_emb1 = zip(sentences, embeddings)\n",
    "sent_emb2 = zip(tokens_dict, embeddings3)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in sent_emb1:\n",
    "    # 转换为1维tensor\n",
    "    embedding = torch.tensor(embedding).unsqueeze(0)\n",
    "    for sentence2, embedding2 in zip(sentences, embeddings):\n",
    "        embedding2 = torch.tensor(embedding2).unsqueeze(0)\n",
    "        # 计算余弦相似度\n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        cos_sim = cos(embedding, embedding2)\n",
    "        print(\"Sentence = \", sentence)\n",
    "        print(\"Sentence2 = \", sentence2)\n",
    "        print(\"Similarity = \", cos_sim.item())\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-11-15 20:53:09,342] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load '/dataQ/zhijian/.paddlenlp/taskflow/information_extraction/uie-base'.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'疾病': [{'end': 3,\n",
      "          'probability': 0.8903937046969972,\n",
      "          'relations': {'': [{'end': 8,\n",
      "                              'probability': 0.8827342624272205,\n",
      "                              'start': 4,\n",
      "                              'text': '易感人群'}]},\n",
      "          'start': 0,\n",
      "          'text': '百日咳'}]}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from paddlenlp import Taskflow\n",
    "\n",
    "medical_data = ['']\n",
    "# ie = Taskflow('information_extraction', schema=medical_data)\n",
    "# pprint(ie(\"治愈率大于30%,感染概率小于50%的疾病有哪些,至少列出3个\"))\n",
    "# print(\"-\" * 42)\n",
    "# pprint(ie(\"大叶性肺炎,二硫化碳中毒的预防措施\"))\n",
    "# print(\"-\" * 42)\n",
    "# pprint(ie(\"百日咳的易感人群和治好概率\"))\n",
    "\n",
    "schema = {\"疾病\": medical_data}\n",
    "ie = Taskflow('information_extraction', schema=schema)\n",
    "pprint(ie(\"百日咳的易感人群和治好概率\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 综合上述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n",
      "[2023-11-15 13:59:00,574] [   DEBUG] _compat.py:47 - Paddle enabled successfully......\n",
      "[2023-11-15 13:59:00,581] [ WARNING] process.py:69 - Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: ',']\n",
      "[2023-11-15 13:59:00,582] [ WARNING] process.py:69 - Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '?']\n",
      "[2023-11-15 13:59:00,584] [ WARNING] SentenceTransformer.py:805 - No sentence-transformers model found with name /dataQ/zhijian/LLMs/m3e-base. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------word segemenataion result--------------------\n",
      "['大叶性肺炎', ',', '二硫化碳中毒', '的', '预防', '措施', '?']\n",
      "-------------------- fuzz --------------------\n",
      "大叶性肺炎 // [('疾病名称', 0), ('疾病描述', 0), ('疾病种类', 0), ('科室', 0), ('病因', 0)]\n",
      ", // [('疾病名称', 0), ('疾病描述', 0), ('疾病种类', 0), ('科室', 0), ('病因', 0)]\n",
      "二硫化碳中毒 // [('疾病名称', 0), ('疾病描述', 0), ('疾病种类', 0), ('科室', 0), ('病因', 0)]\n",
      "的 // [('疾病名称', 0), ('疾病描述', 0), ('疾病种类', 0), ('科室', 0), ('病因', 0)]\n",
      "预防 // [('预防措施', 90), ('疾病名称', 0), ('疾病描述', 0), ('疾病种类', 0), ('科室', 0)]\n",
      "措施 // [('预防措施', 90), ('疾病名称', 0), ('疾病描述', 0), ('疾病种类', 0), ('科室', 0)]\n",
      "? // [('疾病名称', 0), ('疾病描述', 0), ('疾病种类', 0), ('科室', 0), ('病因', 0)]\n",
      "-------------------- similarity --------------------\n",
      "Tokens Similarity Scores:\n",
      "Token: 预防 / 预防措施 / Similarity: 0.9377005100250244\n",
      "Token: 措施 / 预防措施 / Similarity: 0.8163185119628906\n",
      "Token: 大叶性肺炎 / 病因 / Similarity: 0.8127889633178711\n",
      "\n",
      "Sentence Similarity Scores:\n",
      "Sentence: 大叶性肺炎,二硫化碳中毒的预防措施? / 预防措施 / Similarity: 0.8289975523948669\n"
     ]
    }
   ],
   "source": [
    "# 1.jieba participle\n",
    "import jieba\n",
    "import paddle\n",
    "from thefuzz import process\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "def jieba_div(strs: list)->list:\n",
    "    paddle.enable_static()\n",
    "    jieba.enable_paddle()# 启动paddle模式。 0.40版之后开始支持，早期版本不支持\n",
    "    segmented_list = []\n",
    "    for string in strs:\n",
    "        seg_list = jieba.cut(string, use_paddle=True)  # 使用paddle模式\n",
    "        segmented_str = \"/\".join(seg_list)  # 用'/'连接分词结果\n",
    "        segmented_list.append(segmented_str.split('/'))  # 分割字符串并添加到最终列表\n",
    "\n",
    "    print('-'*20 + \" word segemenataion result \" + '-'*20)\n",
    "    print(segmented_list[0])\n",
    "    return segmented_list[0]\n",
    "\n",
    "\n",
    "def fuzz(sents1: list, sents2: list)->list:\n",
    "\n",
    "    # for item1 in sents1:\n",
    "    #     for item2 in sents2:\n",
    "    #         print(\"item1:\",item1,\"item2:\",item2)\n",
    "    #         process.extract(item1, item2, limit=5)\n",
    "    \n",
    "    # change the list to string\n",
    "    # sents1 = ''.join(sents1)\n",
    "    for item1 in sents1:\n",
    "        print(item1,'//',process.extract(item1,sents2, limit=5))\n",
    "\n",
    "\n",
    "def similarity(tokens_input, tokens_dict, model_path, sent):\n",
    "    model = SentenceTransformer(model_path)\n",
    "    tokens_input_emb = [model.encode(token) for token in tokens_input]\n",
    "    tokens_dict_emb = [model.encode(token) for token in tokens_dict]\n",
    "    sent_emb = model.encode(sent)\n",
    "\n",
    "    # 初始化两个列表用于存储相似度得分和对应的token或句子\n",
    "    token_pairs_scores = []\n",
    "    sent_pairs_scores = []\n",
    "\n",
    "    # 计算token对和句子对的余弦相似度\n",
    "    cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    for sentence1, embedding1 in zip(tokens_input, tokens_input_emb):\n",
    "        for sentence2, embedding2 in zip(tokens_dict, tokens_dict_emb):\n",
    "            cos_sim = cos(torch.tensor(embedding1), torch.tensor(embedding2))\n",
    "            if cos_sim.item() > 0.8:\n",
    "                token_pairs_scores.append((sentence1, sentence2, cos_sim.item()))\n",
    "\n",
    "    for sentence2, embedding2 in zip(tokens_dict, tokens_dict_emb):\n",
    "        cos_sent = cos(torch.tensor(sent_emb), torch.tensor(embedding2))\n",
    "        if cos_sent.item() > 0.8:\n",
    "            sent_pairs_scores.append((sent, sentence2, cos_sent.item()))\n",
    "\n",
    "    # 对列表按照相似度得分排序\n",
    "    token_pairs_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "    sent_pairs_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # 打印token对和句子对的相似度得分\n",
    "    print(\"Tokens Similarity Scores:\")\n",
    "    for score in token_pairs_scores:\n",
    "        print(f\"Token: {score[0]} / {score[1]} / Similarity: {score[2]}\")\n",
    "\n",
    "    print(\"\\nSentence Similarity Scores:\")\n",
    "    for score in sent_pairs_scores:\n",
    "        print(f\"Sentence: {score[0]} / {score[1]} / Similarity: {score[2]}\")\n",
    "\n",
    "# tokens_input = ['肺泡蛋白质沉积症', '肺泡蛋白质沉淀' , '肺泡蛋白沉着症', '肺泡沉淀有蛋白质', '放射性肺炎', '肺念珠菌病', '肺大疱', '肺炎球菌肺炎', '肺气肿', '肺炎杆菌肺炎']\n",
    "# tokens_dict = ['易感','容易感染','容易患上','易感人群','容易得']\n",
    "medical_data = ['疾病名称', '疾病描述', '疾病种类', '科室', '病因', '症状', '检查', \n",
    "                '并发症', '花费', '疗程', '疗法', '治愈率', '易感人群', '感染概率', \n",
    "                '感染途径', '预防措施', '推荐药物', '常用药物', '具体药物', '可以吃', \n",
    "                '不可以吃', '推荐吃', '是否纳入医保']\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_path = '/dataQ/zhijian/LLMs/m3e-base'    \n",
    "    input = ['大叶性肺炎,二硫化碳中毒的预防措施?']\n",
    "    tokens_input = jieba_div(input)\n",
    "    tokens_dict = medical_data\n",
    "    \n",
    "    print(\"-\"*20, \"fuzz\", \"-\"*20)\n",
    "    fuzz(tokens_input, tokens_dict)\n",
    "    print(\"-\"*20, \"similarity\", \"-\"*20)   \n",
    "    similarity(tokens_input, tokens_dict, model_path,input[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema过长问题解决\n",
    "1. 重定义一个短的schema页表做映射\n",
    "2. 将schema先进行一遍摘要再存储，每次使用摘要过后的schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
